---
title: 웹의 역사
toc: true
toc_sticky: true
categories:
  - web
date: 2022-02-26
---

## 웹의 역사

### 웹 이전의 인터넷

초기의 인터네에는 웹이 없었다.
인터넷의 기원은 1969년에 구축된 ARPANET 까지 거슬러 올라간다.
ARPANET은 Advanced Research Project Agency에서 구축한 컴퓨터 네트워크다.
ARPANET은 미국 내 대학과 연구기관 사이를 고속 회선으로 접속하고, 전 미국을 연결하는 네트워크로서 서서히 성장해갔다.
웹 이전의 역사는 브라이언 캐니핸이 이시다 교수에게 보낸 전자메일에서 엿 볼수 있는데, 메일의 내용은 일본어임에도 모든 문자가 영문자와 숫자로 되어있다.
그리고 당시의 네트워크는 리얼 타임으로 상대와 통신하는 TCP/IP 뿐만 아니라, 패킷 릴레이 방식의 UUCP에 의한 전송도 존재했었기 때문에 메일이 도달하기까지 지연이 있었다.
즉, 요즘처럼 메일을 보내면 바로 상대방이 받아볼 수 있는 것이 아니었다.
인터넷 애플리케이션은 전자메일 이외에도 많이 생겨났다. 
여러 사람들이 참가할 수 있는 포럼 형태의 넷 뉴스, 파일 교환을 위한 File Transfer Protocol, UNIX 호스트에 원격접속하기 위한 Telnet, 콘텐츠를 간단히 공개하기 위한 Gopher 등이 있다.

<br/>

### 웹 이전의 하이퍼미디어

**Memex - 하이퍼미디어의 기원**
하이퍼미디어의 기원은 ARPANET의 탄생보다 더 거슬로 올라간 1945년에 미국의 연구자 버니바 부시가 발표한 Memex라는 정보 검색 시스템에 대한 노문이라고 한다.
Memex는 실재하는 시스템이 아닌 구상이었지만, 전기적으로 접속한 책과 파일을 서로 링크하고 링크를 따라서 차례로 표시하는 현재의 웹을 예상할 수 있는 시스템이었다.
이 Memex 구상에는 하이퍼미디어라는 단어조차 등장하지 않았지만 많은 연구자들에게 영향을 끼쳤다.



### Xanadu - 하이퍼미디어라는 단어의 탄생

부시의 Memex 구상에 영향을 받았던 연구자 중에 테드 넬슨이 있었다.
넬슨은 1965년에 하이퍼텍스트와 하이퍼미디어라는 말을 잇달아 고안했다.
하이퍼텍스트가 문자 정보 중심의 문서를 상호 링크시키는 개념임에 반해, 하이퍼미디어는 그 사고를 확장하여 음성과 동영상 등 다양한 미디어를 상호 링크시킨 개념이다.
넬슨은 이 단어들을 고안함과 동시에 현재의 웹을 더욱 진화시킨 기능을 가진 이상적인 하이퍼미디어 Xanadu를 구상하고 개발하기 시작했다.
그러나 Xandu의 개발은 고기능으로 인한 복잡성으로 실패하고 말았다.



### HyperCard - 최초의 실용적인 하이퍼미디어

웹 이전에 성공을 거둔 하이퍼미디어는 Bill Atksinson이 1987년에 Apple에서 개발한 HyperCard가 있다. 
HyperCard에는 네트워크를 통해 데이터를 주고받는 기능조차 없었지만, 카드라고 불리는 문서를 단위로 상호 링크하고, 스크립트 언어 HyperTalk에 의한  프로그램을 실행할 수 있는 말하자면, Stand-alone 방식의 웹 서비스였다.
HyperCard는 성공을 거두었고, 많은 게임과 애플리케이션들이 개발되었다.



### 웹 이전의 하이퍼미디어의 문제점

지금까지 가장 많이 보급된 하이퍼미디어의 구현은 웹이다. 웹 상의 문서는 모두가 링크에 의해 서로 연결되어 있다.
링크가 웹에 필수불가결한 기본 기술이라는 사실은 Google의 페이지랭크와 트랙백과 같은 기술이 링크를 전제로 설계되어 있다는 점에서 명확히 알 수 있다.
다만, 넬슨 등 예전의 하이퍼미디어 추진자들의 시각으로 볼 때, 웹은 불완전한 하이퍼미디어로 비춰질 것이다.
그 이유는 웹이 단방향 링크밖에 지원하고 있지 않고, 링크가 끊어질 가능성이 있으며, 버전 관리와 트랜스클루전 기능이 없는 것 등이다.
하지만, 현실에서는 그 보급율로 볼 때 웹이 가장 성공한 하이퍼미디어라는 점은 의심할 여지가 없다.
웹의 성공을 가져온 원인은 최소한의 링크 기능만을 갖추고 있었다는 점이다.
반대로 웹 이전의 하이퍼미디어의 최대 문제점은 그 복잡성에 있었다고 할 수 있다.

<br/>

### 웹 이전의 분산 시스템

**중앙 집중형 시스템과 분산 시스템**

가장 최초의 컴퓨터는 과학기술계산 등의 전용 목적으로 만들어졌다.
그러던 것이 1960년대에 메인 프레임이 개발되면서 한 대의 컴퓨터를 여러 목적으로 이용될 수 있게 되었다.
이 당시 컴퓨터의 이용형태는 단말기로 호스트 컴퓨터에 접속하여 호스트 컴퓨터에서 집중해서 처리하는 방식이었다.
1970년대 이후, 컴퓨터의 다운사이징이 진행되면서 컴퓨터들이 소형화되고 성능은 향상됨에 따라 복수의 컴퓨터를 조합하여 처리를 분산시킴으로서 전체적인 성능을 향상시킬 수 있는 방법들이 등장했다.

**RPC - 다른 컴퓨터의 기능을 이용하기**

분산 시스템을 실현하기 위해서는 각 서버가 제공하는 기능을 다른 서버와 클라이언트에서 호출할 수 있어야 한다.
RPC는 분산 시스템을 실현하기 위한 기술 중 하나이다.
RPC를 이용하면 원격 서버에서 실행하고 있는 프로그램을 클라이언트 쪽으로 호출할 수 있다.
유명한 RPC 시스템으로는 Sun Microsystems의 SunRPC와 아폴로, IBM과 DEC가 공동 개발한 DCE가 있다.
RPC 시스템이 개발되던 1980년대 후반은 UNIX 전쟁이라고 불리는 UNIX 벤더의 의한 표준화 경쟁이 치열하던 시대로, 모두 자사의 분산 시스템 기술을 표준으로 하기 위해 열심이었다.

**CORBA, DCOM - 분산 오브젝트로의 진화**

RPC는 이름 그대로 리모트 프로시저, 즉 함수를 호출하는 구조이다. 다만, 현대적인 프로그래밍 언어들은 거의 모두가 객체지향 기능을 갖추고 있다.
그래서 단순한 함수 호출이 아니라, 오브젝트 자체를 원격으로 배치하는 분산 오브젝트라고 불리는 기술이 고안되었다.
분산 오브젝트의 대표적인 예는 CORBA이다.
Microsoft는 CORBA에 대항해서 DCOM을 개발했다.
CORBA와 DCOM은 Interface Definition Language로 오브젝트의 메서드를 정의하고, 구현은 네트워크를 경유해 시리얼라이즈 된 메시지를 교환하는 점이 RPC와 동일하다.
단, 범용적인 오브젝트 기능을 실현하려고 했기 때문에 매우 복잡한 스펙을 가지게 되었다. 또한 CORBA와 DCOM 은 호환성이 없어서 서로의 시스템이 접속할 수 없다는 문제점이 있었다.

**웹 이전의 분산 시스템의 문제점**
RPC는 지금도 Network File System 같은 분산 시스템을 구현하는데 사용되고 있다.
하지만 RPC가 현실적으로 동작하는 것은 통신상대가 어느 정도 정해져 있는 인트라넷 환경까지로 좀 더 복잡한 이기종 분산 환경으로 확장되지 않는다.
확장 되지 않는 이유로는 RPC 시스템에 몇몇의 문제가 있었기 때문이다.

- 성능열화의 문제

  네트워크를 경유한 함수의 호출은 동일 프로세스 내에서 함수를 호출하는 데 비해 몇 배나 시간이 걸린다.
  또한, 일반적으로 함수의 입도가 작아 목적을 달성하기 위해선 여러 번 호출하지 않으면 안 되고, 네트워크의 오버헤드가 호출하는 회수만큼 걸린다.

- 데이터형 변환의 문제
  프로그래밍 언어마다 지원하는 데이터형이 다르기 때문에 복수의 언어가 혼재하는 환경에서는 데이터형 변환 시 문제가 발생한다.

- 인터페이스 버전업 시 호환성 문제
  기능을 추가하면서 서버의 인터페이스가 변경된 경우, 구 클라이언트에 대해 하위 호환성을 가질 수 없다.

- 부하 분산의 문제
  일반적으로 RPC 기반의 시스템은 서버 상에 클라이언트의 애플리케이션 상태를 가지고 있다.
  그렇기 때문에 서버끼리 애플리케이션 상태를 공유하지 않으면 안 되며, 다수의 서버에서 부하를 분산하는 것이 어려워진다.

이렇게 웹 이전의 분산 시스템은 하드웨어든 소프트웨어든 한정된 수로 균일한 클라이언트를 전제로 했다.
이런 방식으로는 전 세계적인 규모로 동작하는 시스템이 될 수 없었고, 대규모 분산 시스템에 필요한 것은 무엇인지는 웹에 의해서 명확해진다.

<br/>

### 웹의 탄생

지금까지 살펴본 것처럼 1980년대까지 하이퍼미디어에 대한 구상이 생겨나고 인터넷이 등장하면서 복수의 컴퓨터를 연결한 분산 시스템이 구축되었다.
1990년 11월 12일, 스위스의 CERN이라는 국제 연구소에서 근무하던 팀 버너스-리가 하이퍼미디어를 이용한 인터넷 기반의 분산정보관리 시스템이라는 웹 제안서를 썼다.
버너스-리는  첫 버전의 서버와 브라우저를 완성시켰고 웹은 전 세계로 서서히 보급되기 시작했다.
당시의 인터넷은 주로 기업과 대학 연구소가 이용하고 있었는데, 그들은 점차 무상으로 공개된 서버와 브라우저를 시험 삼아 사용하게 되었고 콘텐츠를 공개하기 시작했다.
웹의 보급을 단번에 앞당긴 것이 1993년 일리노이 대학의 NCSA가 공개한 브라우저 Mosaic 이다. 
그전가지의 브라우저가 브라우저 자체로는 문자정보밖에 다루지 못했던 것에 비해, Mosaic은 본문에 인라인으로 이미지를 혼재시킬 수 있다.
Mosaic은 Internet Explore나 FireFox 같은 현재의 브라우저의 원류가 되었다.

**하이퍼미디어로서의 웹**

웹은 인터넷을 이용한 하이퍼미디어로서 설계되었다.
웹 이전의 하이퍼미디어와 가장 큰 차이점으로는 인터넷을 이용하기 때문에 불특정 다수의 정보를 서로 링크시킬 수 있고, 시스템을 대규모화하기 쉽다는 중요한 이점을 가지고 있다.
반면, 정보의 집중적인 관리가 어려워지고 링크가 끊어지기 쉽다는 결점도 가지고 있다.
그리고 웹이 구현하고 있는 링크는 심플한 단방향 링크뿐이라는 점도 특징이다.
웹에서는 바루엊에 표시하거나 링크를 클릭하면 새로운 웹페이지로 이동한다. 하지만, 원래 링크의 개념은 외부에서 링크를 지정하는 확장 링크의 개념도 존재했다.
웹에 복잡한 링크 구조를 도입하려는 움직임도 있었지만, 결국은 심플한 단방향 링크만 사용되고 있다.
사용자에게 있어서 이해하기 쉽고 구현이 간단한 링크였기 때문에 웹이 여기까지 보급되었다고 할 수 있다.s

**분산 시스템으로서의 웹**

RPC는 폐쇄된 네트워크 환경에서 미리 상정한 숫자와 종류의 클라이언트를 상대로 서비스를 제공하는 시스템으로는 뛰어난다.
거꾸로 말하면, 개방된 네트워크 환경에서 불특정 다수의 클라이언트에 대해 서비스를 제공하는 시스템으로는 어울리지 않는다.
개방형이고 불특정 다수를 상대로 하는 시스템이 바로 웹이다.
웹에서는 전 세계의 유저들이 전 세계의 웹 서비스를 이용할 수 있다.
각 유저의 컴퓨터 환경은 특정한 OS와 하드웨어로 통일되어 있지 않으며, 다양한 브라우저와 디바이스를 통해 하나의 웹 서비스를 접근할 수 있다.

<br/>

### 웹의 표준화

Mosaic에 의해 보급된 웹에는 다양한 플레이어가 추가되었다.
학술적인 콘첸츠뿐만 아니라, 뉴스와 오락 미디어의 참여, 쇼핑 사이트의 등장 등 1990년대 중후반에 걸쳐 동시다발적으로 일어났다.

**웹의 스펙 책정**

이런 상황 속에서 웹을 구성하는 기술, HTTP와 URI, HTML에 대한 표준화가 요구되었다.
이들은 각 회사의 서버, 클라이언트 사이에서 이용되어야 하고 상호 운용성이 요구되었기 때문이다.
웹 이전의 인터넷 표준은 모두 IETF의 RFC로 정해왔다. 실제로 HTTP, URI 그리고 버전2까지의 HTML은 RFC로 정의되어 있따.
그러나 웹이 너무나 급속하게 보급되어 버렸기 때문에 IETF에서의 스펙 책정이 따라가질 못하고, 각 기업의 구현은 제각각이라 상호운용성이 결여되는 상태가 발생하게 되었다.
이러한 문제를 해결하기 위해, 웹 기술을 구현하고 있는 베넏들이 모여 표준화를 수행하는 단체로 World Wide Web Consortium를 설립한다.
w3c에서는 html, xml, uri, css, http 등의 표준화 작업이 이루어졌고, 당시의 상황을 브라우저 전쟁이라고도 부른다.
Negtscape Navigator와 Internet Explorer가 독자적인 확장을 반복한 끝에 양쪽 진영의 HTML과 css의 렌더링 결과가 큭레 차이가 나게 되고, 개발자들은 브라우저 별로 대응해야만 하는 사태에 이르게 되었다. 이런 상황은 오랜 시간이 지나면서 서서히 해결되었지만, 현재에도 문제는 남아있다.
웹이 여기까지 확장성을 가지고 동작하고 있는 것은 서버와 브라우저를 구현한 경험과 HTTP나 URI의 스펙이 책정되는 과정에 설계적으로 올바른 선택이 이어져온 결과다.

**REST 탄생**

웹의 아키텍처를 결정한 중요한 인물로는 로이 필딩이라고 한다.
그는  Apache httpd & libwww-perl등 각종 소프트웨어의 구현에 관여해 왔다.
필딩은 이 구현 경험을 바탕으로 버너스-리 그룹과 함께 HTTP 1.0과 HTTP 1.1의 스펙을 제정하는데 관여했다.
HTTP의 스펙을 책정하는 시기에 필딩은 대학원생이기도 했기 때문에, 자신의 연구과제로 웹이 왜 이렇게 성공했는지, 왜 이정도의 대규모 시스템이 성립된것인지에 대해 소프트웨어 아키텍처의 관점에서 분석하고 하나의 아키텍처 스타일로 정리했다.
2000년, 그는 이 아키텍처 스타일을 REST라 이름을 붙이고, 박사학위 논문으로 제출했다.
REST라는 이름은 HTTP의 약자에서 힌트를 얻었다. 즉 HTTP는 원래 하이퍼텍스트를 전송하기 위한 프로토콜이었지만, 실제로는 하이퍼텍스트 이외의 리소스 상태의 표현들을 전송하고 있다고 필딩은 주장했다. 
그렇기 때문에 Representational State Transfer라고 이름 붙였던 것이다.

### 다양한 하이퍼미디어 포맷의 탄생

초기 웹에서는 HTML이 유일한 하이퍼미디어 포맷이었다.
하지만, 웹이 보급됨에 따라 HTML만으로는 대응할 수 없는 다양한 요구가 생겨나기 시작해, 새로운 하이퍼미디어 포맷들이 탄생했다.
예를 들어, HTML의 구조는 그대로 유지한 채, HTML에 다양한 의미를 가지게 할 수 있는 기술로서 microformats가 등장했다.
또한 웹페이지의 새로운 정보를 서버에서 발송하고, 전용 프로그램으로 그것을 체그하기 위한 용도로 RSS가 제안되었다.
그러나 RSS 는 복수의 버전이 난립해 혼란스러웠던 탓ㅊ에 최종적으로는 IETF에서 Atom이 표준화되었다.
Html과 Atom은 xml을 베이스로 한 구조화 문서용 마크업 언어이기 때문에 데이터 기술을 위한 표기가 너무 중복되었고, 그래서 좀 더 단순한 데이터 포맷이 몇 가지 제안되었고, 그중에서 사실상 표준이 된 것이 JSON이다.

<br/>

### 웹 API를 둘러싼 논의

초기의 웹은 학술논문 교환에 이용되어서 주로 사람이 문서를 읽기 위한 시스템이었는데, 웹의 용도가 다양화되면서 프로그램을 자동화 처리를 하고자 하는 요구가 생겨나기 시작했다.
1990년대 후반부터 2000년대 전반에 걸쳐 프로그램으로 조작이 가능한 웹 API에 대한 논의가 일어났다.

**SOAP와 WS-**

1990년대 후반, 웹은 상업적인 성공을 거두고 버블을 맞이하게 되었다. 
웹 기술을 사용하는 것이 트렌드가 되었따.
HTTP 1.1을 책정하는 필딩의 그룹과는 별개로, 다양한 배경을 가진 그룹들이 웹을 프로그램에서 이용할 수 있도록 하기 위해 확장을 시도했다.
그 중에서 큰 세력을 가지고 있던 것이 RPC/분산 오브젝트 그룹이다.
그들은 과거에 CORBA와 DCOM 같은 분산 오브젝트로 자사의 기술을 디팩토 스탠더드로 마들기 위해 표준화 경쟁을 벌인 적이 있다.
그것들은 거의 성공하지 못했지만, 같은 방법론으로 웹상의 분산 오브젝트를 구현하려고 했다.

RFC/분산 오브젝트 그룹의 움직임 중에서 가장 기본적인 프로토콜은 SOAP이다.
SOAP은 http를 애플리케이션 프로토콜이 아닌 트랜스포트 프로토콜로 다루고, HTTP 상에서 독자적으로 메시지를 전송한다.
SOAP은 Microsoft가 w3c에 제안하고, IBM과 그 밖의 벤더를 끌어들여 표준화가 시작되었다.
SOAP는 메시지 전송 방법만을 규정한 스펙이기 때문에 실제로 시스템을 구축할 때는 SOAP 상에 서비스 별로 프로토콜을 정의하지 않으면 안된다.
이것들을 각 벤더마다 제각기 정의하게 되면 이전의 분산 시스템의 전철을 밟는 셈이었기 때문에 ws-security, ws- transaction 등 ws- 라고 불리는 주변 스펙들이 w3c와 oasis에 제안되었다.
하지만 여러 비슷비슷한 스펙이 어려 개가 난립했기 때문에, 결국 표준화 경쟁을 불러일으켰다.

**SOAP 대 REST**

이런 SOAP와 ws-를 둘러싼 혼란 속에서 당시 W3C의 메일링 리스트에서는 프로그램에서도 이용 가능한 웹의 아키텍처에 대해서 활발히 논의되었다.
이 논쟁에는 필딩도 적극적으로 관여했다.
그는 직접 만든 REST의 이론을 바탕으로 대기업들이 추진하는 SOAP 기반의 기술을 부정하고, 웹이 웹다울 수 있는 아키텍처로서 REST를 권장했다.
필딩이 SOAP의 잘못된 점을 아무리 지적햇도 soap 스펙의 책정 작업은 W3C에서 계속 진행되었다.
하지만, 필딩의 의견을 지지하는 사람들이 서서히 나타났다. 그 대표격이라고 할 수 있는 사람이 마크베이커와 폴 프레스코드이다.
베이커는 분산 오브젝트 엔지니어로, 웹 이전부터 분산 오브젝트 기술에 관여하고 있었다.
프레스코드는 XML/구조화 문서 엔지니어다. 이렇게 기술적 배경이 다른 두사람이 웹을 통해 REST를 만났고, 다양한 미디어를 통해 함께 REST를 선전했다.

**REST의 오해와 보급**

SOAP와 REST에 관한 논쟁은 2000년대 전후부터 시작되어 2003년 정도가 정점이었다.
2000년 당시는 Google이 검색엔진으로서 겨우 일정한 지위를 가지기 시작할 무렵으로, 현재와 같은 각종 웹 API는 존재하지 않았다.
REST의 보급이 탄력을 받기 시작한 것은 2002년에 등장한 Amazon 웹 서비스다.
Amazon은 자신들이 취급하는 서적과 그 밖의 상품들의 정보를 웹을 통해 프로그램으로 취급할 수 있도록 했다.
그 때 Amazon은 SOAP를 이용한 형식과 특정 URI와 HTTP로 GET하는 형식의 2가지를 준비했다.
기술적으로 정확한 것은 아니지만, 후자를 편의상 REST 형식이라고 불렀다.
Amazon의 웹 API는 그 정보의 유용성과 편리한 사용법에 힘입어 순식간에 보급되었다.
그리고 SOAP와 REST의 이용 비율이 20대 80이라고 보고 되자, SOAP 대 REST의 논쟁에 불이 붙었다.
REST를 부정하는 사람들의 주장은 Amazon처럼 보안이 필요없는 간단한 웹 API에서는 URI를 GET하기만 하는 단순한 방식이 이용될 수 있지만 기간 시스템 같은 트랜잭션과 신뢰성이 필요한 곳에는 REST의 기능은 불충분하다는 것이었다.
REST 대 SOAP의 논쟁은 열기를 더하고 REST는 http와 URI 만으로 기간 시스템을 만들 수 있냐? REST는 장난감이다라고 멸시까지 했다.
하지만 결과적으로 REST측이 승리했고 웹 2.0 흐름 속에서 Google과 Amazon 같은 기업들은 REST 형식의 웹 API를 제공하기 시작했다.
웹 2.0에서 중요했던 것은 매쉬업이다. 매쉬업이란 여러 가지 웹 API가 제공하는 정보를 조합해 하나의 애플리케이션을 실현하는 방법을 말한다.
매쉬업에서는 가벼움이 요구되었기 때문에, 웹 API가 제공하는 리소스를 HTTP와 URI로 간단히 조작할 수 있는 REST 스타일 쪽이 받아들여졌던 것이다.

**SOAP와 WS- 패인**

첫째는 기술적인 이유다.
SOAP와 WS-는  RPC/분산 오브젝트가 가지고 있던 기술적인 문제점을 그대로 가지고 있는데다 스펙들마저 복잡해저 버렸다.
예를 들어, 벤더 간 인터페이스 호환성의 결여, 복잡한 프로토콜 스택, 네트워크를 통한 인터페이스 호출에 의한 오버헤드 등이다.

둘째는 정치적인 이유다.
SOAP와 WS- 의 표준화 작업은 W3C와 OASIS에서 수행했다.
여기서의 표준화 작업은 각 벤더가 드래프트를 가지고 오면, 그 차이를 조정하는 식으로 이루어졌다.
하지만 많은 벤더들이 SOAP 자체도 표준으로 확정되기도 전에 구현을 추진했기 때문에 동일한 SOAP와 WS-라도 해석에 차이가 생겼고 호환성이 결여되었다.

<br/>

### 모든 것은 웹으로

REST가 보급되면서 웹은 인터넷을 통째로 집어 삼키기 시작했다.
그때까지는 별도의 프로토콜을 사용하던 메일과 넷 뉴스의 경우 백엔드에서 동작하고 있는 프로토콜은 변화하지 않았지만, 적어도 유저 인터페이스는 웹으로 통일하기 시작했고 엔드 유저는 웹만을 인식하게 되었다.
이런 배경에는 Ajax와 Comet 등의 기술적 돌파구가 있다.
이 기술들에 의해서, 그전까지 있을 수 없었던 사용자 인터페이스와 편의성이 웹의 장접과 맞물려 계속 실현되었다.
예를 들어 예전의 맵 소프트는 맵 데이터를 로컬 하드디스크에 설치해 이용했었다. 
하지만 이 방식으로는 한 대의 PC에 인스톨 한 데이터밖에는 다룰 수 없다.
그에 비해, 현재의 맵 서비스는 전 세계의 위성사진과 지도를 언제라도 최신 상태로 이용할 수 있다.
이것은 서버 측에서 맵 데이터를 모아서 보관하고, 필요에 따라 웹을 통해 다운로드하는 방식이기 때문에 실현될 수 있는 것이다.
현재 우리들이 이용하고 있는 소프트웨어의 많은 부분이 웹을 전제로 하고 있고 모든 소프트웨어와 데이터들이 계속해서 웹으로 구현되면서 웹의 중요성은 날로 커지고 있다.